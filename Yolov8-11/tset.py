# import torch
# import torch.nn as nn

# # Polar Coordinate Circular Dynamic Snake Convolutionæåæ ‡ç¯å½¢åŠ¨æ€è›‡å½¢å·ç§¯
# __all__ = ['C3k2_PCCDSConv']


# def autopad(k, p=None, d=1):  # kernel, padding, dilation
#     """Pad to 'same' shape outputs."""
#     if d > 1:
#         k = d * (k - 1) + 1 if isinstance(k, int) else [d * (x - 1) + 1 for x in k]  # actual kernel-size
#     if p is None:
#         p = k // 2 if isinstance(k, int) else [x // 2 for x in k]  # auto-pad
#     return p


# class Conv(nn.Module):
#     """Standard convolution with args(ch_in, ch_out, kernel, stride, padding, groups, dilation, activation)."""
#     default_act = nn.SiLU()  # default activation

#     def __init__(self, c1, c2, k=1, s=1, p=None, g=1, d=1, act=True):
#         """Initialize Conv layer with given arguments including activation."""
#         super().__init__()
#         self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p, d), groups=g, dilation=d, bias=False)
#         self.bn = nn.BatchNorm2d(c2)
#         self.act = self.default_act if act is True else act if isinstance(act, nn.Module) else nn.Identity()

#     def forward(self, x):
#         """Apply convolution, batch normalization and activation to input tensor."""
#         return self.act(self.bn(self.conv(x)))

#     def forward_fuse(self, x):
#         """Perform transposed convolution of 2D data."""
#         return self.act(self.conv(x))


# class DySnakeConv(nn.Module):
#     def __init__(self, inc, ouc, k=3) -> None:
#         super().__init__()

#         self.conv_0 = Conv(inc, ouc, k)
#         self.conv_x = DSConv(inc, ouc, 0, k)
#         self.conv_y = DSConv(inc, ouc, 1, k)

#     def forward(self, x):
#         return torch.cat([self.conv_0(x), self.conv_x(x), self.conv_y(x)], dim=1)


# class DSConv(nn.Module):
#     def __init__(self, in_ch, out_ch, morph, kernel_size=3, if_offset=True):
#         """
#         The Dynamic Snake Convolution
#         :param in_ch: input channel
#         :param out_ch: output channel
#         :param kernel_size: the size of kernel
#         :param morph: the morphology of the convolution kernel is mainly divided into two types
#                         along the x-axis (0) and the y-axis (1) (see the paper for details)
#         :param if_offset: whether deformation is required, if it is False, it is the standard convolution kernel
#         """
#         super(DSConv, self).__init__()
#         # use the <offset_conv> to learn the deformable offset
#         self.offset_conv = nn.Conv2d(in_ch, 2 * kernel_size, 3, padding=1)
#         self.bn = nn.BatchNorm2d(2 * kernel_size)
#         self.kernel_size = kernel_size

#         # two types of the DSConv (along x-axis and y-axis)
#         self.dsc_conv_x = nn.Conv2d(
#             in_ch,
#             out_ch,
#             kernel_size=(kernel_size, 1),
#             stride=(kernel_size, 1),
#             padding=0,
#         )
#         self.dsc_conv_y = nn.Conv2d(
#             in_ch,
#             out_ch,
#             kernel_size=(1, kernel_size),
#             stride=(1, kernel_size),
#             padding=0,
#         )

#         self.gn = nn.GroupNorm(out_ch // 4, out_ch)
#         self.act = Conv.default_act

#         self.morph = morph
#         self.if_offset = if_offset

#         # å¯å­¦ä¹ å‚æ•°
#         self.radius_elastic = nn.Parameter(torch.tensor(1.0))
#         self.extend_scope = nn.Parameter(torch.tensor(1.0))

#     def forward(self, f):
#         # å‚æ•°çº¦æŸ
#         self.radius_elastic.data = torch.clamp(self.radius_elastic.data, min=0.1, max=10.0)
#         self.extend_scope.data = torch.clamp(self.extend_scope.data, min=0.1, max=10.0)

#         offset = self.offset_conv(f)
#         offset = self.bn(offset)
#         # We need a range of deformation between -1 and 1 to mimic the snake's swing
#         offset = torch.tanh(offset)
#         input_shape = f.shape
#         dsc = DSC(input_shape, self.kernel_size, self.extend_scope, self.morph, self.radius_elastic)
#         deformed_feature = dsc.deform_conv(f, offset, self.if_offset)
#         if self.morph == 0:
#             x = self.dsc_conv_x(deformed_feature.type(f.dtype))
#             x = self.gn(x)
#             x = self.act(x)
#             return x
#         else:
#             x = self.dsc_conv_y(deformed_feature.type(f.dtype))
#             x = self.gn(x)
#             x = self.act(x)
#             return x


# # Core code, for ease of understanding, we mark the dimensions of input and output next to the code
# class DSC(object):
#     def __init__(self, input_shape, kernel_size, extend_scope, morph, radius_elastic):
#         self.num_points = kernel_size
#         self.width = input_shape[2]
#         self.height = input_shape[3]
#         self.morph = morph
#         self.extend_scope = extend_scope  # offset (-1 ~ 1) * extend_scope
#         self.radius_elastic = radius_elastic

#         # define feature map shape
#         """
#         B: Batch size  C: Channel  W: Width  H: Height
#         """
#         self.num_batch = input_shape[0]
#         self.num_channels = input_shape[1]

#     """
#     input: offset [B,2*K,W,H]  K: Kernel size (2*K: 2D image, deformation contains <x_offset> and <y_offset>)
#     output_x: [B,1,W,K*H]   coordinate map
#     output_y: [B,1,K*W,H]   coordinate map
#     """

#     def _coordinate_map_3D(self, offset, if_offset):
#         device = offset.device
#         # offset
#         y_offset, x_offset = torch.split(offset, self.num_points, dim=1)

#         y_center = torch.arange(0, self.width).repeat([self.height])
#         y_center = y_center.reshape(self.height, self.width)
#         y_center = y_center.permute(1, 0)
#         y_center = y_center.reshape([-1, self.width, self.height])
#         y_center = y_center.repeat([self.num_points, 1, 1]).float()
#         y_center = y_center.unsqueeze(0)

#         x_center = torch.arange(0, self.height).repeat([self.width])
#         x_center = x_center.reshape(self.width, self.height)
#         x_center = x_center.permute(0, 1)
#         x_center = x_center.reshape([-1, self.width, self.height])
#         x_center = x_center.repeat([self.num_points, 1, 1]).float()
#         x_center = x_center.unsqueeze(0)

#         if self.morph == 0:
#             """
#             Initialize the kernel and flatten the kernel
#                 y: only need 0
#                 x: -num_points//2 ~ num_points//2 (Determined by the kernel size)
#                 !!! The related PPT will be submitted later, and the PPT will contain the whole changes of each step
#             """
#             y = torch.linspace(0, 0, 1)
#             x = torch.linspace(
#                 -int(self.num_points // 2),
#                 int(self.num_points // 2),
#                 int(self.num_points),
#             )

#             y, x = torch.meshgrid(y, x, indexing='ij')
#             y_spread = y.reshape(-1, 1)
#             x_spread = x.reshape(-1, 1)

#             y_grid = y_spread.repeat([1, self.width * self.height])
#             y_grid = y_grid.reshape([self.num_points, self.width, self.height])
#             y_grid = y_grid.unsqueeze(0)  # [B*K*K, W,H]

#             x_grid = x_spread.repeat([1, self.width * self.height])
#             x_grid = x_grid.reshape([self.num_points, self.width, self.height])
#             x_grid = x_grid.unsqueeze(0)  # [B*K*K, W,H]

#             y_new = y_center + y_grid
#             x_new = x_center + x_grid

#             y_new = y_new.repeat(self.num_batch, 1, 1, 1).to(device)
#             x_new = x_new.repeat(self.num_batch, 1, 1, 1).to(device)

#             y_offset_new = y_offset.detach().clone()

#             if if_offset:
#                 y_offset = y_offset.permute(1, 0, 2, 3)
#                 y_offset_new = y_offset_new.permute(1, 0, 2, 3)
#                 center = int(self.num_points // 2)

#                 # The center position remains unchanged and the rest of the positions begin to swing
#                 # This part is quite simple. The main idea is that "offset is an iterative process"
#                 y_offset_new[center] = 0
#                 for index in range(1, center):
#                     y_offset_new[center + index] = (y_offset_new[center + index - 1] + y_offset[center + index])
#                     y_offset_new[center - index] = (y_offset_new[center - index + 1] + y_offset[center - index])
#                 y_offset_new = y_offset_new.permute(1, 0, 2, 3).to(device)
#                 y_new = y_new.add(y_offset_new.mul(self.extend_scope))

#             # Convert to polar coordinates
#             r = torch.sqrt(x_new ** 2 + y_new ** 2)
#             theta = torch.atan2(y_new, x_new)

#             # Radius elastic adjustment
#             r = r * self.radius_elastic

#             # Harmonic angle optimization
#             r = r + torch.sin(4 * theta) * self.extend_scope

#             # Dynamic range adjustment
#             dynamic_scope = torch.mean(torch.abs(offset), dim=(1, 2, 3), keepdim=True)
#             r = r + dynamic_scope * torch.sin(theta) * self.extend_scope

#             # Convert back to Cartesian coordinates
#             x_new = r * torch.cos(theta)
#             y_new = r * torch.sin(theta)

#             y_new = y_new.reshape(
#                 [self.num_batch, self.num_points, 1, self.width, self.height])
#             y_new = y_new.permute(0, 3, 1, 4, 2)
#             y_new = y_new.reshape([
#                 self.num_batch, self.num_points * self.width, 1 * self.height
#             ])
#             x_new = x_new.reshape(
#                 [self.num_batch, self.num_points, 1, self.width, self.height])
#             x_new = x_new.permute(0, 3, 1, 4, 2)
#             x_new = x_new.reshape([
#                 self.num_batch, self.num_points * self.width, 1 * self.height
#             ])
#             return y_new, x_new

#         else:
#             """
#             Initialize the kernel and flatten the kernel
#                 y: -num_points//2 ~ num_points//2 (Determined by the kernel size)
#                 x: only need 0
#             """
#             y = torch.linspace(
#                 -int(self.num_points // 2),
#                 int(self.num_points // 2),
#                 int(self.num_points),
#             )
#             x = torch.linspace(0, 0, 1)

#             y, x = torch.meshgrid(y, x, indexing='ij')
#             y_spread = y.reshape(-1, 1)
#             x_spread = x.reshape(-1, 1)

#             y_grid = y_spread.repeat([1, self.width * self.height])
#             y_grid = y_grid.reshape([self.num_points, self.width, self.height])
#             y_grid = y_grid.unsqueeze(0)

#             x_grid = x_spread.repeat([1, self.width * self.height])
#             x_grid = x_grid.reshape([self.num_points, self.width, self.height])
#             x_grid = x_grid.unsqueeze(0)

#             y_new = y_center + y_grid
#             x_new = x_center + x_grid

#             y_new = y_new.repeat(self.num_batch, 1, 1, 1)
#             x_new = x_new.repeat(self.num_batch, 1, 1, 1)

#             y_new = y_new.to(device)
#             x_new = x_new.to(device)
#             x_offset_new = x_offset.detach().clone()

#             if if_offset:
#                 x_offset = x_offset.permute(1, 0, 2, 3)
#                 x_offset_new = x_offset_new.permute(1, 0, 2, 3)
#                 center = int(self.num_points // 2)
#                 x_offset_new[center] = 0
#                 for index in range(1, center):
#                     x_offset_new[center + index] = (x_offset_new[center + index - 1] + x_offset[center + index])
#                     x_offset_new[center - index] = (x_offset_new[center - index + 1] + x_offset[center - index])
#                 x_offset_new = x_offset_new.permute(1, 0, 2, 3).to(device)
#                 x_new = x_new.add(x_offset_new.mul(self.extend_scope))

#             # Convert to polar coordinates
#             r = torch.sqrt(x_new ** 2 + y_new ** 2)
#             theta = torch.atan2(y_new, x_new)

#             # Radius elastic adjustment
#             r = r * self.radius_elastic

#             # Harmonic angle optimization
#             r = r + torch.sin(4 * theta) * self.extend_scope

#             # Dynamic range adjustment
#             dynamic_scope = torch.mean(torch.abs(offset), dim=(1, 2, 3), keepdim=True)
#             r = r + dynamic_scope * torch.sin(theta) * self.extend_scope

#             # Convert back to Cartesian coordinates
#             x_new = r * torch.cos(theta)
#             y_new = r * torch.sin(theta)

#             y_new = y_new.reshape(
#                 [self.num_batch, 1, self.num_points, self.width, self.height])
#             y_new = y_new.permute(0, 3, 1, 4, 2)
#             y_new = y_new.reshape([
#                 self.num_batch, 1 * self.width, self.num_points * self.height
#             ])
#             x_new = x_new.reshape(
#                 [self.num_batch, 1, self.num_points, self.width, self.height])
#             x_new = x_new.permute(0, 3, 1, 4, 2)
#             x_new = x_new.reshape([
#                 self.num_batch, 1 * self.width, self.num_points * self.height
#             ])
#             return y_new, x_new

#     """
#     input: input feature map [N,C,D,W,H]ï¼›coordinate map [N,K*D,K*W,K*H] 
#     output: [N,1,K*D,K*W,K*H]  deformed feature map
#     """

#     def _bilinear_interpolate_3D(self, input_feature, y, x):
#         device = input_feature.device
#         y = y.reshape([-1]).float()
#         x = x.reshape([-1]).float()

#         zero = torch.zeros([]).int()
#         max_y = self.width - 1
#         max_x = self.height - 1

#         # find 8 grid locations
#         y0 = torch.floor(y).int()
#         y1 = y0 + 1
#         x0 = torch.floor(x).int()
#         x1 = x0 + 1

#         # clip out coordinates exceeding feature map volume
#         y0 = torch.clamp(y0, zero, max_y)
#         y1 = torch.clamp(y1, zero, max_y)
#         x0 = torch.clamp(x0, zero, max_x)
#         x1 = torch.clamp(x1, zero, max_x)

#         input_feature_flat = input_feature.flatten()
#         input_feature_flat = input_feature_flat.reshape(
#             self.num_batch, self.num_channels, self.width, self.height)
#         input_feature_flat = input_feature_flat.permute(0, 2, 3, 1)
#         input_feature_flat = input_feature_flat.reshape(-1, self.num_channels)
#         dimension = self.height * self.width

#         base = torch.arange(self.num_batch) * dimension
#         base = base.reshape([-1, 1]).float()

#         repeat = torch.ones([self.num_points * self.width * self.height
#                              ]).unsqueeze(0)
#         repeat = repeat.float()

#         base = torch.matmul(base, repeat)
#         base = base.reshape([-1])

#         base = base.to(device)

#         base_y0 = base + y0 * self.height
#         base_y1 = base + y1 * self.height

#         # top rectangle of the neighbourhood volume
#         index_a0 = base_y0 - base + x0
#         index_c0 = base_y0 - base + x1

#         # bottom rectangle of the neighbourhood volume
#         index_a1 = base_y1 - base + x0
#         index_c1 = base_y1 - base + x1

#         # get 8 grid values
#         value_a0 = input_feature_flat[index_a0.type(torch.int64)].to(device)
#         value_c0 = input_feature_flat[index_c0.type(torch.int64)].to(device)
#         value_a1 = input_feature_flat[index_a1.type(torch.int64)].to(device)
#         value_c1 = input_feature_flat[index_c1.type(torch.int64)].to(device)

#         # find 8 grid locations
#         y0 = torch.floor(y).int()
#         y1 = y0 + 1
#         x0 = torch.floor(x).int()
#         x1 = x0 + 1

#         # clip out coordinates exceeding feature map volume
#         y0 = torch.clamp(y0, zero, max_y + 1)
#         y1 = torch.clamp(y1, zero, max_y + 1)
#         x0 = torch.clamp(x0, zero, max_x + 1)
#         x1 = torch.clamp(x1, zero, max_x + 1)

#         x0_float = x0.float()
#         x1_float = x1.float()
#         y0_float = y0.float()
#         y1_float = y1.float()

#         vol_a0 = ((y1_float - y) * (x1_float - x)).unsqueeze(-1).to(device)
#         vol_c0 = ((y1_float - y) * (x - x0_float)).unsqueeze(-1).to(device)
#         vol_a1 = ((y - y0_float) * (x1_float - x)).unsqueeze(-1).to(device)
#         vol_c1 = ((y - y0_float) * (x - x0_float)).unsqueeze(-1).to(device)

#         outputs = (value_a0 * vol_a0 + value_c0 * vol_c0 + value_a1 * vol_a1 +
#                    value_c1 * vol_c1)

#         if self.morph == 0:
#             outputs = outputs.reshape([
#                 self.num_batch,
#                 self.num_points * self.width,
#                 1 * self.height,
#                 self.num_channels,
#             ])
#             outputs = outputs.permute(0, 3, 1, 2)
#         else:
#             outputs = outputs.reshape([
#                 self.num_batch,
#                 1 * self.width,
#                 self.num_points * self.height,
#                 self.num_channels,
#             ])
#             outputs = outputs.permute(0, 3, 1, 2)
#         return outputs

#     def deform_conv(self, input, offset, if_offset):
#         y, x = self._coordinate_map_3D(offset, if_offset)
#         deformed_feature = self._bilinear_interpolate_3D(input, y, x)
#         return deformed_feature


# class Bottleneck(nn.Module):
#     """Standard bottleneck."""

#     def __init__(self, c1, c2, shortcut=True, g=1, k=(3, 3), e=0.5):
#         """Initializes a bottleneck module with given input/output channels, shortcut option, group, kernels, and
#         expansion.
#         """
#         super().__init__()
#         c_ = int(c2 * e)  # hidden channels
#         self.cv1 = Conv(c1, c_, k[0], 1)
#         self.cv2 = Conv(c_, c2, k[1], 1, g=g)
#         self.add = shortcut and c1 == c2

#     def forward(self, x):
#         """'forward()' applies the YOLO FPN to input data."""
#         return x + self.cv2(self.cv1(x)) if self.add else self.cv2(self.cv1(x))


# class Bottleneck_DySnakeConv(Bottleneck):
#     """Standard bottleneck with DySnakeConv."""

#     def __init__(self, c1, c2, shortcut=True, g=1, k=(3, 3), e=0.5):  # ch_in, ch_out, shortcut, groups, kernels, expand
#         super().__init__(c1, c2, shortcut, g, k, e)
#         c_ = int(c2 * e)  # hidden channels
#         self.cv2 = DySnakeConv(c_, c2, k[1])
#         self.cv3 = Conv(c2 * 3, c2, k=1)

#     def forward(self, x):
#         """'forward()' applies the YOLOv5 FPN to input data."""
#         return x + self.cv3(self.cv2(self.cv1(x))) if self.add else self.cv3(self.cv2(self.cv1(x)))


# class C2f(nn.Module):
#     """Faster Implementation of CSP Bottleneck with 2 convolutions."""

#     def __init__(self, c1, c2, n=1, shortcut=False, g=1, e=0.5):
#         """Initializes a CSP bottleneck with 2 convolutions and n Bottleneck blocks for faster processing."""
#         super().__init__()
#         self.c = int(c2 * e)  # hidden channels
#         self.cv1 = Conv(c1, 2 * self.c, 1, 1)
#         self.cv2 = Conv((2 + n) * self.c, c2, 1)  # optional act=FReLU(c2)
#         self.m = nn.ModuleList(Bottleneck(self.c, self.c, shortcut, g, k=((3, 3), (3, 3)), e=1.0) for _ in range(n))

#     def forward(self, x):
#         """Forward pass through C2f layer."""
#         y = list(self.cv1(x).chunk(2, 1))
#         y.extend(m(y[-1]) for m in self.m)
#         return self.cv2(torch.cat(y, 1))

#     def forward_split(self, x):
#         """Forward pass using split() instead of chunk()."""
#         y = list(self.cv1(x).split((self.c, self.c), 1))
#         y.extend(m(y[-1]) for m in self.m)
#         return self.cv2(torch.cat(y, 1))


# class C3(nn.Module):
#     """CSP Bottleneck with 3 convolutions."""

#     def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):
#         """Initialize the CSP Bottleneck with given channels, number, shortcut, groups, and expansion values."""
#         super().__init__()
#         c_ = int(c2 * e)  # hidden channels
#         self.cv1 = Conv(c1, c_, 1, 1)
#         self.cv2 = Conv(c1, c_, 1, 1)
#         self.cv3 = Conv(2 * c_, c2, 1)  # optional act=FReLU(c2)
#         self.m = nn.Sequential(*(Bottleneck(c_, c_, shortcut, g, k=((1, 1), (3, 3)), e=1.0) for _ in range(n)))

#     def forward(self, x):
#         """Forward pass through the CSP bottleneck with 2 convolutions."""
#         return self.cv3(torch.cat((self.m(self.cv1(x)), self.cv2(x)), 1))


# class C3k_DSConv(C3):
#     """C3k is a CSP bottleneck module with customizable kernel sizes for feature extraction in neural networks."""

#     def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5, k=3):
#         """Initializes the C3k module with specified channels, number of layers, and configurations."""
#         super().__init__(c1, c2, n, shortcut, g, e)
#         c_ = int(c2 * e)  # hidden channels
#         # self.m = nn.Sequential(*(RepBottleneck(c_, c_, shortcut, g, k=(k, k), e=1.0) for _ in range(n)))
#         self.m = nn.Sequential(*(Bottleneck_DySnakeConv(c_, c_, shortcut, g, k=(k, k), e=1.0) for _ in range(n)))


# class C3k2_PCCDSConv(C2f):
#     """Faster Implementation of CSP Bottleneck with 2 convolutions."""

#     def __init__(self, c1, c2, n=1, c3k=False, e=0.5, g=1, shortcut=True):
#         """Initializes the C3k2 module, a faster CSP Bottleneck with 2 convolutions and optional C3k blocks."""
#         super().__init__(c1, c2, n, shortcut, g, e)
#         self.m = nn.ModuleList(
#             C3k_DSConv(self.c, self.c, 2, shortcut, g) if c3k else Bottleneck(self.c, self.c, shortcut, g) for _ in
#             range(n)
#         )
#     # åœ¨ç‰¹å¾æå–æ—¶ç”¨DSConv,åœ¨è¾…åŠ©ç‰¹å¾èåˆæ—¶æ¢å›åŸå…ˆçš„Bottleneck


# if __name__ == "__main__":
#     # Generating Sample image
#     image_size = (1, 64, 240, 240)
#     image = torch.rand(*image_size)

#     # Model
#     mobilenet_v1 = C3k2_PCCDSConv(64, 64, c3k=True)

#     out = mobilenet_v1(image)
#     print(out.size())


import argparse
from pathlib import Path
import torch
import numpy as np
from PIL import Image, ImageDraw, ImageFont
import warnings
import glob
import os
import cv2
from packaging import version

# å¿½ç•¥ä¸å¿…è¦çš„è­¦å‘Š
warnings.filterwarnings("ignore", category=UserWarning)

try:
    from ultralytics.models.yolo.detect.val import DetectionValidator
    from ultralytics import YOLO, __version__
    print(f"âœ… å¯¼å…¥æˆåŠŸ, Ultralytics v{__version__}")
except ImportError:
    print("âš ï¸ æœªèƒ½ä»ultralyticså¯¼å…¥DetectionValidatorï¼Œå°è¯•ä»æœ¬åœ°æ–‡ä»¶å¯¼å…¥")

# ç‰ˆæœ¬æ£€æŸ¥
if version.parse(__version__) < version.parse("8.0.0"):
    raise EnvironmentError("éœ€è¦ Ultralytics 8.0.0 æˆ–æ›´é«˜ç‰ˆæœ¬")

def draw_custom(image, boxes, scores, labels, conf_thres):
    """
    è‡ªå®šä¹‰ç»˜åˆ¶å‡½æ•°ï¼Œä½¿ç”¨æŒ‡å®šçš„é¢œè‰²å’Œå­—ä½“æ ·å¼æ ‡æ³¨æ£€æµ‹ç»“æœ
    
    å‚æ•°:
        image (PIL.Image): åŸå§‹å›¾åƒå¯¹è±¡
        boxes (list): è¾¹ç•Œæ¡†åæ ‡åˆ—è¡¨
        scores (list): ç½®ä¿¡åº¦åˆ†æ•°åˆ—è¡¨
        labels (list): ç±»åˆ«æ ‡ç­¾åˆ—è¡¨
        conf_thres (float): ç½®ä¿¡åº¦é˜ˆå€¼
    """
    # å®šä¹‰ç±»åˆ«åç§°æ˜ å°„
    class_names = {
        0: "pedestrian",
        1: "people",
        2: "bicycle",
        3: "car",
        4: "van",
        5: "truck",
        6: "tricycle",
        7: "awning-tricycle",
        8: "bus",
        9: "motor"
    }
    
    # å®šä¹‰ç±»åˆ«é¢œè‰²æ˜ å°„
    class_colors = {
        0: (255, 0, 0),        # çº¢è‰² - pedestrian
        1: (0, 255, 0),        # ç»¿è‰² - people
        2: (0, 0, 255),        # è“è‰² - bicycle
        3: (230, 180, 80),     # é»„è‰² - car
        4: (255, 0, 255),      # ç´«è‰² - van
        5: (0, 255, 255),      # é’è‰² - truck
        6: (255, 165, 0),      # æ©™è‰² - tricycle
        7: (128, 0, 128),      # æ·±ç´«è‰² - awning-tricycle
        8: (0, 128, 128),     # è“ç»¿è‰² - bus
        9: (128, 128, 0),      # æ©„æ¦„è‰² - motor
    }
    
    # é»˜è®¤é¢œè‰²ï¼ˆå½“ç±»åˆ«è¶…å‡ºå®šä¹‰èŒƒå›´æ—¶ä½¿ç”¨ï¼‰
    default_color = (200, 200, 200)  # ç°è‰²
    
    draw = ImageDraw.Draw(image)
    
    # å°è¯•åŠ è½½å­—ä½“ï¼ˆWindowsç³»ç»Ÿé€šå¸¸æœ‰Arialå­—ä½“ï¼‰
    try:
        # æŸ¥æ‰¾ç³»ç»Ÿå­—ä½“ï¼ˆä¼˜å…ˆä½¿ç”¨Arialï¼‰
        font_path = None
        possible_fonts = [
            "arial.ttf", 
            "arialbd.ttf", 
            "ariblk.ttf",
            "C:/Windows/Fonts/arial.ttf",
            "C:/Windows/Fonts/arialbd.ttf"
        ]
        
        for font_candidate in possible_fonts:
            if os.path.exists(font_candidate):
                font_path = font_candidate
                break
        
        if font_path:
            # æ ¹æ®å›¾åƒå°ºå¯¸åŠ¨æ€è®¾ç½®å­—ä½“å¤§å° - ç¼©å°å­—ä½“å°ºå¯¸
            min_dim = min(image.width, image.height)
            # å‡å°‘å­—ä½“å¤§å°æ¯”ä¾‹
            font_size = max(10, int(min_dim * 0.02))  # ä»0.03å‡å°åˆ°0.02
            font = ImageFont.truetype(font_path, font_size)
        else:
            # å°è¯•ä½¿ç”¨PILå†…ç½®å­—ä½“
            font = ImageFont.load_default()
    except:
        font = ImageFont.load_default()
    
    # æŒ‰ç½®ä¿¡åº¦æ’åºï¼Œç¡®ä¿é«˜ç½®ä¿¡åº¦çš„ç›®æ ‡æ˜¾ç¤ºåœ¨ä¸Šå±‚
    indices = np.argsort(scores)[::-1]
    
    for idx in indices:
        if scores[idx] < conf_thres:
            continue
            
        b = boxes[idx]
        label_id = int(labels[idx])
        
        # è·å–ç±»åˆ«åç§°ï¼ˆå¦‚æœæ‰¾ä¸åˆ°åˆ™æ˜¾ç¤º"Unknown"ï¼‰
        class_name = class_names.get(label_id, f"Class {label_id}")
        
        # è·å–ç±»åˆ«é¢œè‰²
        color = class_colors.get(label_id, default_color)
        
        # ç»˜åˆ¶è¾¹æ¡† - å‡å°è¾¹æ¡†å®½åº¦
        draw.rectangle(
            list(b),
            outline=color,
            width=2  # ä»3å‡å°åˆ°2
        )
        
        # å‡†å¤‡æ–‡æœ¬ï¼ˆä½¿ç”¨ç±»åˆ«åç§°å’Œç½®ä¿¡åº¦ï¼‰
        text = f"{class_name}: {scores[idx]:.2f}"
        
        # è·å–æ–‡æœ¬å°ºå¯¸
        try:
            text_bbox = draw.textbbox((0, 0), text, font=font)
            text_width = text_bbox[2] - text_bbox[0]
            text_height = text_bbox[3] - text_bbox[1]
        except:
            # æ—§ç‰ˆPILä½¿ç”¨ä¸åŒæ–¹æ³•
            text_bbox = font.getbbox(text)
            text_width = text_bbox[2] - text_bbox[0]
            text_height = text_bbox[3] - text_bbox[1]
        
        # ç¡®ä¿æ–‡æœ¬åœ¨å›¾åƒèŒƒå›´å†… - å‡å°è¾¹è·
        text_x = max(3, min(b[0], image.width - text_width - 3))
        text_y = max(3, min(b[1] - text_height - 3, image.height - text_height - 3))
        
        # ç»˜åˆ¶æ–‡æœ¬èƒŒæ™¯çŸ©å½¢ - å‡å°å†…è¾¹è·
        bg_rect = [
            text_x - 1,  # ä»-2å‡å°åˆ°-1
            text_y - 1,  # ä»-2å‡å°åˆ°-1
            text_x + text_width + 1,  # ä»+2å‡å°åˆ°+1
            text_y + text_height + 1  # ä»+2å‡å°åˆ°+1
        ]
        draw.rectangle(
            bg_rect,
            fill=color
        )
        
        # ç»˜åˆ¶æ–‡æœ¬
        draw.text(
            (text_x, text_y),
            text,
            font=font,
            fill=(255, 255, 255)  # ç™½è‰²æ–‡æœ¬
        )
    
    return image

def detect_and_visualize(model_path, image_paths, output_dir="detections", imgsz=640, conf_thres=0.25, device='', 
                         measure_fps=False, fps_image=None, fps_iterations=100):
    """
    åŠ è½½YOLOv11æ¨¡å‹ï¼Œå¯¹å›¾ç‰‡/è§†é¢‘è¿›è¡Œç›®æ ‡æ£€æµ‹å¹¶å¯è§†åŒ–ç»“æœ
    
    å‚æ•°:
        model_path (str): æ¨¡å‹æƒé‡æ–‡ä»¶è·¯å¾„
        image_paths (list): å¾…æ£€æµ‹å›¾ç‰‡/è§†é¢‘è·¯å¾„åˆ—è¡¨
        output_dir (str): ç»“æœä¿å­˜ç›®å½•
        imgsz (int): è¾“å…¥å°ºå¯¸
        conf_thres (float): ç½®ä¿¡åº¦é˜ˆå€¼
        device (str): æŒ‡å®šè®¡ç®—è®¾å¤‡
        measure_fps (bool): æ˜¯å¦æ‰§è¡Œä¸“ä¸šFPSæµ‹é‡
        fps_image (str): FPSæµ‹é‡ç”¨çš„å›¾åƒè·¯å¾„
        fps_iterations (int): FPSæµ‹é‡çš„è¿­ä»£æ¬¡æ•°
    """
    # æ”¯æŒçš„å›¾åƒå’Œè§†é¢‘æ ¼å¼
    supported_image_suffix = ['.png', '.jpg', '.jpeg', '.bmp', '.tif', '.tiff', '.webp']
    supported_video_suffix = ['.mp4', '.avi', '.mov', '.mkv', '.gif']
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_path = Path(output_dir)
    output_path.mkdir(exist_ok=True, parents=True)
    
    # åŠ è½½æ¨¡å‹
    try:
        model = YOLO(model_path)
        if device:
            model = model.to(device)
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ, è®¾å¤‡: {model.device.type}")
    except RuntimeError as e:
        if 'CUDA out of memory' in str(e):
            print("âŒ æ˜¾å­˜ä¸è¶³! å°è¯•å‡å°imgszå€¼æˆ–ä½¿ç”¨--half")
        else:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return
    except Exception as e:
        print(f"âŒ æœªçŸ¥é”™è¯¯: {e}")
        return
    
    # ==============================================
    # ä¸“ä¸šFPSæµ‹é‡åŠŸèƒ½
    # ==============================================
    def measure_fps(model, image, num_iterations=100, warmup_iterations=10):
        """
        æ‰§è¡Œä¸“ä¸šçº§çš„FPSæµ‹é‡ï¼ŒåŒ…æ‹¬é¢„çƒ­å’Œç»Ÿè®¡è®¡ç®—
        
        å‚æ•°:
            model (YOLO): åŠ è½½çš„YOLOæ¨¡å‹
            image (np.array): è¾“å…¥å›¾åƒ(numpyæ•°ç»„)
            num_iterations (int): æµ‹é‡è¿­ä»£æ¬¡æ•°
            warmup_iterations (int): GPUé¢„çƒ­è¿­ä»£æ¬¡æ•°
            
        è¿”å›:
            dict: åŒ…å«è¯¦ç»†æ€§èƒ½æŒ‡æ ‡çš„å­—å…¸
        """
        import time
        import numpy as np
        
        # é¢„çƒ­GPU
        print(f"ğŸ”¥ é¢„çƒ­GPU ({warmup_iterations}æ¬¡è¿­ä»£)...")
        for _ in range(warmup_iterations):
            model.predict(image, verbose=False)
        
        # åˆå§‹åŒ–è®¡æ—¶å™¨
        total_preprocess = []
        total_inference = []
        total_postprocess = []
        total_fps = []
        
        # ä¸»æµ‹é‡å¾ªç¯
        print(f"ğŸ“Š å¼€å§‹FPSæµ‹é‡ ({num_iterations}æ¬¡è¿­ä»£)...")
        for i in range(num_iterations):
            # è®°å½•å¼€å§‹æ—¶é—´
            start_time = time.perf_counter()
            
            # æ‰§è¡Œæ¨ç†
            results = model.predict(image, verbose=False)
            result = results[0]  # åªå–ç¬¬ä¸€ä¸ªç»“æœ
            
            # è®¡ç®—å„é˜¶æ®µæ—¶é—´
            if hasattr(result, 'speed'):
                speed = result.speed
                pre_time = speed.get('preprocess', 0)
                inf_time = speed.get('inference', 0)
                post_time = speed.get('postprocess', 0)
                total_time = pre_time + inf_time + post_time
                
                total_preprocess.append(pre_time)
                total_inference.append(inf_time)
                total_postprocess.append(post_time)
                total_fps.append(1000 / total_time if total_time > 0 else float('inf'))
        
        # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
        stats = {
            'preprocess_avg': np.mean(total_preprocess) if total_preprocess else 0,
            'preprocess_std': np.std(total_preprocess) if total_preprocess else 0,
            'inference_avg': np.mean(total_inference) if total_inference else 0,
            'inference_std': np.std(total_inference) if total_inference else 0,
            'postprocess_avg': np.mean(total_postprocess) if total_postprocess else 0,
            'postprocess_std': np.std(total_postprocess) if total_postprocess else 0,
            'fps_avg': np.mean(total_fps) if total_fps else 0,
            'fps_std': np.std(total_fps) if total_fps else 0,
            'min_latency': min(total_preprocess) + min(total_inference) + min(total_postprocess),
            'max_latency': max(total_preprocess) + max(total_inference) + max(total_postprocess),
            'iterations': num_iterations,
            'warmup': warmup_iterations,
            # æ·»åŠ èŒƒå›´æ•°æ®åˆ°è¿”å›çš„å­—å…¸ä¸­
            'preprocess_min': min(total_preprocess) if total_preprocess else 0,
            'preprocess_max': max(total_preprocess) if total_preprocess else 0,
            'inference_min': min(total_inference) if total_inference else 0,
            'inference_max': max(total_inference) if total_inference else 0,
            'postprocess_min': min(total_postprocess) if total_postprocess else 0,
            'postprocess_max': max(total_postprocess) if total_postprocess else 0
        }
        
        # è®¡ç®—æ€»ä½“å»¶è¿Ÿ
        total_latencies = [p + i + po for p, i, po in zip(total_preprocess, total_inference, total_postprocess)]
        stats['latency_avg'] = np.mean(total_latencies)
        stats['latency_std'] = np.std(total_latencies)
        stats['p95_latency'] = np.percentile(total_latencies, 95)
        
        return stats
    
    # å¦‚æœå¯ç”¨äº†FPSæµ‹é‡
    if measure_fps:
        print("\nğŸ”¬ å¼€å§‹ä¸“ä¸šçº§FPSæµ‹é‡...")
        
        # å‡†å¤‡æµ‹è¯•å›¾åƒ
        test_image = None
        if fps_image:
            try:
                img = Image.open(fps_image).convert('RGB')
                test_image = np.array(img)
                print(f"ğŸ“· ä½¿ç”¨æŒ‡å®šå›¾åƒè¿›è¡ŒFPSæµ‹é‡: {fps_image}")
            except Exception as e:
                print(f"âš ï¸ æ— æ³•åŠ è½½æŒ‡å®šå›¾åƒ: {e}")
                test_image = None
        
        # å¦‚æœæœªæŒ‡å®šå›¾åƒæˆ–åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å›¾åƒ
        if test_image is None:
            # å°è¯•ä»è¾“å…¥è·¯å¾„è·å–ç¬¬ä¸€å¼ å›¾åƒ
            if image_paths:
                try:
                    img = Image.open(image_paths[0]).convert('RGB')
                    test_image = np.array(img)
                    print(f"ğŸ“· ä½¿ç”¨è¾“å…¥è·¯å¾„çš„ç¬¬ä¸€å¼ å›¾åƒè¿›è¡ŒFPSæµ‹é‡")
                except:
                    # ç”Ÿæˆéšæœºæµ‹è¯•å›¾åƒ
                    test_image = np.random.randint(0, 255, (imgsz, imgsz, 3), dtype=np.uint8)
                    print("ğŸ“· ä½¿ç”¨éšæœºç”Ÿæˆçš„å›¾åƒè¿›è¡ŒFPSæµ‹é‡")
            else:
                # ç”Ÿæˆéšæœºæµ‹è¯•å›¾åƒ
                test_image = np.random.randint(0, 255, (imgsz, imgsz, 3), dtype=np.uint8)
                print("ğŸ“· ä½¿ç”¨éšæœºç”Ÿæˆçš„å›¾åƒè¿›è¡ŒFPSæµ‹é‡")
        
        # è®¾ç½®æµ‹é‡å‚æ•°
        warmup = max(10, fps_iterations // 10)  # é¢„çƒ­æ¬¡æ•°ä¸ºæ€»æ¬¡æ•°çš„10%
        
        # æ‰§è¡Œæµ‹é‡
        fps_stats = measure_fps(model, test_image, fps_iterations, warmup)
        
        # æ‰“å°ç»“æœ
        print("\n" + "="*70)
        print(f"{'FPS Measurement Results':^70}")
        print("="*70)
        print(f"{'Stage':<15}{'Avg (ms)':>10}{'Std Dev':>10}{'Range (ms)':>15}")
        print(f"{'-'*70}")
        
        # ä½¿ç”¨fps_statså­—å…¸ä¸­çš„å€¼
        preprocess_range = f"{fps_stats['preprocess_min']:.2f}-{fps_stats['preprocess_max']:.2f}"
        inference_range = f"{fps_stats['inference_min']:.2f}-{fps_stats['inference_max']:.2f}"
        postprocess_range = f"{fps_stats['postprocess_min']:.2f}-{fps_stats['postprocess_max']:.2f}"
        total_range = f"{fps_stats['min_latency']:.2f}-{fps_stats['max_latency']:.2f}"
        
        print(f"{'Preprocess':<15}{fps_stats['preprocess_avg']:10.2f}{fps_stats['preprocess_std']:10.2f}{preprocess_range:>15}")
        print(f"{'Inference':<15}{fps_stats['inference_avg']:10.2f}{fps_stats['inference_std']:10.2f}{inference_range:>15}")
        print(f"{'Postprocess':<15}{fps_stats['postprocess_avg']:10.2f}{fps_stats['postprocess_std']:10.2f}{postprocess_range:>15}")
        print(f"{'-'*70}")
        print(f"{'Total Latency':<15}{fps_stats['latency_avg']:10.2f}{fps_stats['latency_std']:10.2f}{total_range:>15}")
        print(f"{'FPS':<15}{fps_stats['fps_avg']:10.2f}{fps_stats['fps_std']:10.2f}")
        print(f"{'95% Latency':<15}{fps_stats['p95_latency']:10.2f}ms")
        print("="*70)
        print(f"æµ‹é‡è¿­ä»£æ¬¡æ•°: {fps_iterations} | é¢„çƒ­è¿­ä»£æ¬¡æ•°: {warmup}")
        
        # æå‰è¿”å›ï¼Œä¸å†æ‰§è¡Œå¸¸è§„æ£€æµ‹
        return
    
    # ==============================================
    # å¸¸è§„æ£€æµ‹å’Œå¯è§†åŒ–åŠŸèƒ½
    # ==============================================
    
    # å¤„ç†æ¯ä¸ªè¾“å…¥æ–‡ä»¶
    valid_paths = []
    for path in image_paths:
        # å¤„ç†é€šé…ç¬¦è·¯å¾„
        expanded_paths = glob.glob(path, recursive=True)
        if not expanded_paths:
            print(f"âš ï¸ æœªæ‰¾åˆ°åŒ¹é…çš„æ–‡ä»¶: {path}")
            continue
            
        for expanded_path in expanded_paths:
            file_path = Path(expanded_path)
            if not file_path.exists():
                print(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                continue
                
            suffix = file_path.suffix.lower()
            if suffix in supported_image_suffix or suffix in supported_video_suffix:
                valid_paths.append(str(file_path))
            else:
                print(f"âš ï¸ ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_path} (ä»…æ”¯æŒ {supported_image_suffix + supported_video_suffix})")
    
    if not valid_paths:
        print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„æ–‡ä»¶å¯å¤„ç†")
        return
        
    print(f"ğŸ” å¼€å§‹æ£€æµ‹: {len(valid_paths)}ä¸ªæœ‰æ•ˆæ–‡ä»¶")
    print(f"ğŸ¯ ç½®ä¿¡åº¦é˜ˆå€¼: {conf_thres}")
    
    for i, path in enumerate(valid_paths):
        print(f"\nğŸ“„ å¤„ç† {i+1}/{len(valid_paths)}: {Path(path).name}")
        
        # æ‰§è¡Œæ¨ç†
        try:
            results = model.predict(
                path, 
                imgsz=imgsz,
                conf=conf_thres,
                save=False,
                save_txt=False,
                save_conf=False,
                save_crop=False,
            )
        except Exception as e:
            print(f"âŒ é¢„æµ‹å¤±è´¥: {e}")
            continue
        
        # å¤„ç†æ¯ä¸ªç»“æœ (è§†é¢‘å¯èƒ½æœ‰å¤šä¸ªå¸§)
        for j, result in enumerate(results):
            # è·å–åŸå§‹å›¾åƒï¼ˆè½¬æ¢ä¸ºPILæ ¼å¼ï¼‰
            if hasattr(result, 'orig_img'):
                orig_img = result.orig_img
                if orig_img is not None:
                    # å°†BGRè½¬ä¸ºRGB
                    orig_img_rgb = cv2.cvtColor(orig_img, cv2.COLOR_BGR2RGB)
                    img_pil = Image.fromarray(orig_img_rgb)
                else:
                    # å°è¯•ç›´æ¥åŠ è½½å›¾åƒ
                    try:
                        img_pil = Image.open(path).convert("RGB")
                    except:
                        print(f"âš ï¸ æ— æ³•åŠ è½½å›¾åƒ: {path}")
                        continue
            else:
                print(f"âš ï¸ ç»“æœå¯¹è±¡æ²¡æœ‰orig_imgå±æ€§")
                continue
            
            # æå–æ£€æµ‹ç»“æœ
            boxes = []
            scores = []
            labels = []
            
            if result.boxes is not None and len(result.boxes):
                for bbox, cls, score in zip(result.boxes.xyxy.cpu().numpy(), 
                                           result.boxes.cls.cpu().numpy(), 
                                           result.boxes.conf.cpu().numpy()):
                    if score < conf_thres:
                        continue
                        
                    boxes.append(bbox)
                    scores.append(score)
                    labels.append(cls)
                
                # æ‰“å°æ£€æµ‹ç»Ÿè®¡
                print(f"ğŸ“Š æ£€æµ‹åˆ°ç›®æ ‡æ•°: {len(boxes)}")
                unique_labels, counts = np.unique(labels, return_counts=True)
                
                for cls_id, count in zip(unique_labels, counts):
                    cls_id = int(cls_id)
                    # ä½¿ç”¨è‡ªå®šä¹‰ç±»å
                    class_names = [
                        "pedestrian", "people", "bicycle", "car", "van",
                        "truck", "tricycle", "awning-tricycle", "bus", "motor"
                    ]
                    class_name = class_names[cls_id] if cls_id < len(class_names) else f"Class_{cls_id}"
                    print(f"  {class_name}: {count}ä¸ª")
            else:
                print("âš ï¸ æœªæ£€æµ‹åˆ°ä»»ä½•ç›®æ ‡")
                boxes = []
                scores = []
                labels = []
            
            # ä½¿ç”¨è‡ªå®šä¹‰ç»˜åˆ¶å‡½æ•°
            try:
                annotated_img = draw_custom(
                    img_pil.copy(), 
                    boxes, 
                    scores, 
                    labels, 
                    conf_thres
                )
                
                # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
                stem = Path(path).stem
                suffix = f"_{j}" if len(results) > 1 else ""
                output_file = output_path / f"{stem}{suffix}_detect.jpg"
                
                # ä¿å­˜å¯è§†åŒ–ç»“æœ
                annotated_img.save(output_file)
                print(f"ğŸ’¾ ç»“æœä¿å­˜è‡³: {output_file}")
                
            except Exception as e:
                print(f"âŒ è‡ªå®šä¹‰ç»˜åˆ¶å¤±è´¥: {e}")
                continue
            
            if hasattr(result, 'speed'):
                speed = result.speed
                total_time = speed.get('preprocess', 0) + speed.get('inference', 0) + speed.get('postprocess', 0)
                fps = 1000 / total_time if total_time > 0 else float('inf')
                
                print(f"âš¡ é€Ÿåº¦ - é¢„: {speed.get('preprocess', 0):.2f}ms, "
                    f"æ¨: {speed.get('inference', 0):.2f}ms, "
                    f"å: {speed.get('postprocess', 0):.2f}ms, "
                    f"æ€»æ—¶é—´: {total_time:.2f}ms, FPS: {fps:.2f}")
                
    print("\nâœ… æ‰€æœ‰æ–‡ä»¶å¤„ç†å®Œæˆ!")

def validate(model_path, data_yaml, imgsz=640):
    """æ‰§è¡ŒYOLOv11åœ¨VisDroneä¸Šçš„éªŒè¯"""
    args = {
        'model': model_path,   # æ¨¡å‹æƒé‡è·¯å¾„
        'data': data_yaml,     # æ•°æ®é›†é…ç½®æ–‡ä»¶
        'batch': 16,           # ä¸è®­ç»ƒä¸€è‡´çš„batch size
        'imgsz': imgsz,        # ä¸è®­ç»ƒä¸€è‡´çš„å›¾åƒå°ºå¯¸
        'conf': 0.001,         # VisDroneä¸“ç”¨ï¼šä½ç½®ä¿¡åº¦é˜ˆå€¼æ£€æµ‹å°ç›®æ ‡
        'iou': 0.6,            # é€‚åˆå¯†é›†åœºæ™¯çš„IoUé˜ˆå€¼
        'device': 'cuda' if torch.cuda.is_available() else 'cpu',
        'name': 'YOLOv11-VisDrone-Val',
        'exist_ok': True,      # è¦†ç›–å·²æœ‰ç»“æœ
        'save_json': True,     # ä¿å­˜COCOæ ¼å¼ç»“æœ
        'save_txt': True,      # ä¿å­˜é¢„æµ‹ç»“æœæ–‡æœ¬
        'task': 'detect',      # æ£€æµ‹ä»»åŠ¡
        'agnostic_nms': True,  # VisDroneä¸“ç”¨ï¼šç±»åˆ«æ— å…³çš„NMS
        'max_det': 300,        # VisDroneä¸“ç”¨ï¼šæ¯å¼ å›¾æœ€å¤šæ£€æµ‹ç›®æ ‡æ•°
        'plots': True,         # ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
        'half': True,          # åŠç²¾åº¦æ¨ç†
        'workers': 4,          # ä¸è®­ç»ƒä¸€è‡´çš„æ•°æ®åŠ è½½çº¿ç¨‹æ•°
        'single_cls': False,   # å¤šç±»åˆ«æ£€æµ‹
    }
    
    print(f"ğŸš€ å¼€å§‹éªŒè¯ YOLOv11 åœ¨ VisDrone ä¸Šçš„æ€§èƒ½")
    print(f"ğŸ“¦ æ¨¡å‹æƒé‡: {model_path}")
    print(f"ğŸ“Š æ•°æ®é›†é…ç½®: {data_yaml}")
    
    try:
        validator = DetectionValidator(args=args)
        validator()  # æ‰§è¡ŒéªŒè¯
    except Exception as e:
        print(f"âŒ éªŒè¯å™¨æ‰§è¡Œå¤±è´¥: {e}")
        return None, None
    
    # ç¡®ä¿metricså¯¹è±¡å­˜åœ¨
    if hasattr(validator, 'metrics') and validator.metrics is not None:
        return validator, validator.metrics.results_dict
    else:
        raise RuntimeError("éªŒè¯å™¨æœªç”Ÿæˆæœ‰æ•ˆçš„metricsç»“æœ")

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='YOLOv11 VisDrone éªŒè¯ä¸æ£€æµ‹')
    
    parser.add_argument('--weights', type=str,
                        default=r"E:/Learning/æ·±åº¦å­¦ä¹ /YoLoç³»åˆ—/v11/runs/train/YOLOv11-VisDrone25/weights/best.pt", 
                        help='æ¨¡å‹æƒé‡è·¯å¾„')
    
    parser.add_argument('--data', type=str, 
                        default=r"E:/Learning/æ·±åº¦å­¦ä¹ /YoLoç³»åˆ—/v11/ultralytics/cfg/datasets/VisDrone.yaml",
                        help='VisDroneæ•°æ®é›†YAMLè·¯å¾„')
    
    parser.add_argument('--imgsz', type=int, default=640,
                        help='å›¾åƒå°ºå¯¸')
                        
    # å›¾ç‰‡/è§†é¢‘æ£€æµ‹å‚æ•°
    parser.add_argument('--source', type=str, action='append', default=[],
                        help='æ£€æµ‹å›¾ç‰‡/è§†é¢‘è·¯å¾„ï¼ˆæ”¯æŒé€šé…ç¬¦å¦‚*.jpgï¼‰')
                        
    parser.add_argument('--conf', type=float, default=0.6,
                        help='æ£€æµ‹ç½®ä¿¡åº¦é˜ˆå€¼')
    
    parser.add_argument('--device', type=str, default='cuda',
                        help='æŒ‡å®šè®¡ç®—è®¾å¤‡ (å¦‚: 0,1 æˆ– "cpu")')
    
    args = parser.parse_args()
    
    # å¦‚æœæ²¡æœ‰æä¾›sourceå‚æ•°ï¼Œä½¿ç”¨é»˜è®¤å€¼
    if not args.source:
        args.source = ["E:/è®ºæ–‡å†™ä½œ/æ¬£èµåˆ«äººçš„è®ºæ–‡/ç›®æ ‡æ£€æµ‹è®ºæ–‡/RT-DETR/è®ºæ–‡å›¾/æ£€æµ‹å›¾/9999952_00000_d_0000238.jpg"]
        # args.source = []
    
    # æ‰§è¡Œæ¨¡å¼é€‰æ‹©
    if args.source:
        # å•å¼ å›¾ç‰‡/è§†é¢‘æ£€æµ‹æ¨¡å¼
        detect_and_visualize(
            model_path=args.weights,
            image_paths=args.source,
            imgsz=args.imgsz,
            conf_thres=args.conf,
            device=args.device if args.device else None
        )
    else:
        # å®Œæ•´éªŒè¯é›†è¯„ä¼°æ¨¡å¼
        try:
            validator, results = validate(args.weights, args.data, args.imgsz)
        except Exception as e:
            print(f"âŒ éªŒè¯è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            exit(1)
        
        # æ£€æŸ¥ç»“æœæ˜¯å¦æœ‰æ•ˆ
        if not results:
            print("âŒ æœªè·å¾—æœ‰æ•ˆçš„éªŒè¯ç»“æœ")
            exit(1)
            
        # æ‰“å°å…³é”®æŒ‡æ ‡
        print("\nğŸ“Š éªŒè¯ç»“æœ:")
        if hasattr(validator.metrics, 'keys') and validator.metrics.keys:
            for key in validator.metrics.keys:
                value = results.get(key, float('nan'))
                print(f"{key}: {value:.4f}")
        else:
            for key, value in results.items():
                print(f"{key}: {value:.4f}")

        # ä¿å­˜ç»“æœåˆ°æ–‡æœ¬æ–‡ä»¶
        result_file = Path(args.weights).parent.parent / 'validation_results.txt'
        try:
            with open(result_file, 'w', encoding='utf-8') as f:
                # ä¿å­˜é€Ÿåº¦ä¿¡æ¯
                if hasattr(validator.metrics, 'speed'):
                    speed = validator.metrics.speed
                    total_time = speed.get('preprocess', 0) + speed.get('inference', 0) + speed.get('postprocess', 0)
                    
                    if total_time > 0:
                        total_fps = 1000 / total_time
                        f.write(f"ç«¯åˆ°ç«¯FPS: {total_fps:.2f}\n")
                    
                    # åˆ†åˆ«è®°å½•å„é˜¶æ®µè€—æ—¶
                    for stage in ['preprocess', 'inference', 'postprocess']:
                        time_ms = speed.get(stage, 'N/A')
                        f.write(f"{stage}æ—¶é—´: {time_ms:.2f}ms/img\n")
                
                # ä¿å­˜æŒ‡æ ‡ç»“æœ
                if hasattr(validator.metrics, 'keys') and validator.metrics.keys:
                    for key in validator.metrics.keys:
                        value = results.get(key, float('nan'))
                        f.write(f"{key}: {value:.4f}\n")
                else:
                    for key, value in results.items():
                        f.write(f"{key}: {value:.4f}\n")
                        
            print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜è‡³: {result_file}")
            
        except Exception as e:
            print(f"âŒ ä¿å­˜ç»“æœæ—¶å‡ºé”™: {e}")